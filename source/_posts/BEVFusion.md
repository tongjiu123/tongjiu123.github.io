---
title: BEVFusion
date: 2023-07-22
categories:
  - Paper
tags:
  - Auto_vehicle
---

# BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework

<!-- more -->

**整体内容概述：**融合激光雷达和相机的信息已经变成了3D目标检测的一个标准，当前的方法依赖于激光雷达传感器的点云作为查询，以利用图像空间的特征。然而，人们发现，这种基本假设使得当前的融合框架无法在发生 LiDAR 故障时做出任何预测，无论是轻微还是严重。这从根本上限制了实际场景下的部署能力。相比之下，作者提出了一个令人惊讶的简单而新颖的融合框架，称为 BEVFusion，其相机流不依赖于 LiDAR 数据的输入，从而解决了以前方法的缺点。作者的框架在正常训练设置下超越了最先进的方法。在模拟各种 LiDAR 故障的鲁棒性训练设置下，作者的框架显着超过了最先进的方法15.7%到28.9%的mAP。这是第一个处理LiDAR故障的方法，并且可以在没有任何后处理程序的情况下部署到实际场景中。代码已经开源。

代码：[https://github.com/ADLab-AutoDrive/BEVFusion](https://link.zhihu.com/?target=https://github.com/ADLab-AutoDrive/BEVFusion)

论文：[https://arxiv.org/abs/2205.13790](https://link.zhihu.com/?target=https://arxiv.org/abs/2205.13790)

**一. 背景介绍**

基于视觉的感知任务，例如3D目标检测，一直是自动驾驶任务的一个关键点。在传统车载视觉感知系统的所有传感器中，激光雷达和摄像头通常是提供准确点云和周围世界图像特征的两个最关键的传感器。在感知系统的早期阶段，人们为每个传感器设计单独的深度模型，并通过后处理方法融合信息。一般来说，汽车无法飞行，所以人们发现生成鸟瞰图 (BEV) 已成为自动驾驶场景的标准。然而，由于缺乏深度信息，通常很难在纯图像输入上回归3D 边界框，同样，当LiDAR没有接收到足够的点时，也很难对点云上的对象进行分类。即：图像缺少激光雷达，则没有深度信息，而激光雷达缺少图像，难以进行目标识别。

最近，人们设计了激光雷达和相机融合的深度网络，以更好地利用两种模式的信息。具体来说，大部分工作可以总结如下：i）给定一个或几个LiDAR点云中的点、LiDAR到世界坐标系的变换矩阵以及相机到世界坐标系的变换矩阵；ii) 人们将 LiDAR 点或proposal转换为相机世界并将其用作查询（queries），以选择对应的图像特征。这条工作线构成了最先进的 3D BEV 感知方法。然而，人们忽略的一个基本假设是，由于需要从LiDAR点生成图像查询，当前的 LiDAR-相机融合方法本质上依赖于LiDAR传感器的原始点云，如图1所示。

![框架对比](v2-cce4bb7c1206e8a5707756c6dadc9276_720w.webp)

图1 框架对比。以前的融合方法可以大致分为 (a) 点级point-level融合机制，将图像特征投影到原始点云上，即找到点云和图像特征对应的部分，融合信息，以及 (b) 特征级融合机制，分别在每个视图图像上投影LiDAR特征或proposal以提取RGB信息。(c) 相比之下，作者提出一个新框架，相机和lidar的输入分开

作者认为，LiDAR和相机融合的理想框架应该是，无论彼此是否存在，单个模态的每个模型都不应该失败，但同时拥有两种模态将进一步提高感知准确性。为此，作者提出了一个令人惊讶的简单而有效的框架，它解决了当前方法的LiDAR相机融合的依赖性，称为BEVFusion。

**二、相关的工作**

在这里，作者根据输入模式对3D检测方法进行广泛分类。

仅仅基于相机模态的3D检测方法、仅仅基于激光雷达的3D检测方法、以及激光雷达和相机融合的3D目标检测方法。当前融合机制的一个被忽视的假设是它们严重依赖LiDAR点云，事实上，如果缺少LiDAR 输入，这些方法将不可避免地失败。

**三．BEVFusion：一个通用的激光雷达和相机融合的框架**

如图2所示，作者详细介绍了他们提出的用于3D目标检测的框架BEVFusion。

![BEVFusion框架](v2-bb4f4c955179af52582870499e12401d_720w.webp)

图2 BEVFusion框架。两个流分别提取特征并将它们转换到相同的BEV空间：i）将相机视图特征投影到3D车身坐标以生成相机BEV特征；ii) 3D backbone从点云中提取LiDAR BEV特征。然后融合两种模态的BEV特征。最后，基于融合的BEV特征构建特定任务的头部，并预测3D目标。

3.1 相机流架构：从多视图图像到BEV空间

作者从Lift-Splat-Shoot (LSS)开始，适度调整LSS以提高性能。相机图像编码、相机视角投视模块等。

![公式](v2-d806be2b954e739f32796ca789f48258_720w.png)

![视图投影](v2-4dc01a78c5cd0935b03104cb4be0952a_720w.webp)

3.4 检测头

由于作者框架的最后一个功能是在BEV空间中，可以利用早期论文中流行的检测头模块。

**四．实验**

在本节中，作者展示了实验设置和BEVFusion的性能。

![泛化能力](v2-506b8997447c42adc617252181acafb5_720w.webp)

表1 BEVFusion的泛化能力。

4.2 泛化能力

BEVFusion框架可以显着提高这些仅LiDAR方法的性能。融合方案将PointPillars提高了18.4% mAP和10.6% NDS，CenterPoint和TransFusion-L提高了3.0%~7.1% mAP。

4.3 和其他方法的比较

![测试集结果](v2-3d9033089d60c2ac281732ab1ce51d81_720w.webp)

表2 上面是验证集结果，下面是测试集结果

4.4 鲁棒性实验

![鲁棒性可视化](v2-dc859c8c5f9d7834d8671aa8197cf0b3_720w.webp)

图4 BEV视角下可视化点云

![有限LiDAR视场结果](v2-0356c639ecdc0810cbf9893388cd0932_720w.webp)

表3 有限LiDAR视场鲁棒性设置的结果。

![目标失败案例结果](v2-8b8bd0c0f1fc31162cc348553de65f97_720w.webp)

表4 目标失败案例的鲁棒性设置结果。

4.4.2 对相机故障的鲁棒性

![相机故障结果](v2-792f2698d32ed7e303783757918af7b3_720w.webp)

表5 相机故障案例的鲁棒性设置结果。

4.5 消融分析

![相机流消融](v2-4edfabd0e0919a3ecbf8eddd43bb2ea8_720w.webp)

表6 相机流消融分析

![融合模块消融](v2-d17648d126a0db9451fb39430e527da6_720w.webp)

表7 融合模块消融分析

**五 总结**

在本文中，作者介绍了BEVFusion，这是一个非常简单但独特的 LiDAR 相机融合框架，它解开了之前方法对LiDAR相机融合的依赖性。作者的框架包括两个独立的流，它们将原始相机和LiDAR传感器输入编码到同一BEV空间中的特征中，然后是一个简单的模块来融合这些特征，以便它们可以传递到现代任务预测头架构中。广泛的实验证明了作者的框架对各种相机和激光雷达故障的强大鲁棒性和泛化能力。
